Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	kraken_summarize_species
	1

[Wed Sep  7 15:38:56 2022]
rule kraken_summarize_species:
    input: atavide.out/statistics/ready_to_summarize
    output: atavide.out/ReadAnnotations/SRR1237783/kraken/SRR1237783.kraken_species_rarefaction.tsv
    jobid: 0
    wildcards: sample=SRR1237783
    resources: mem_mb=2000, time=1440, load_superfocus=0, load_kraken=0, load_onehundred=0


        echo -e "Fraction	SRR1237783" > atavide.out/ReadAnnotations/SRR1237783/kraken/SRR1237783.kraken_species_rarefaction.tsv;
        for FRX in 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9; do awk -v F=$FRX '$4 == "S" {c++} END {print F,"	",c}' atavide.out/ReadAnnotations/SRR1237783/kraken/SRR1237783.report.$FRX.tsv; done >> atavide.out/ReadAnnotations/SRR1237783/kraken/SRR1237783.kraken_species_rarefaction.tsv;
        awk '$4 == "S" {c++} END {print "1.0	",c}' atavide.out/ReadAnnotations/SRR1237783/kraken/SRR1237783.report.tsv >> atavide.out/ReadAnnotations/SRR1237783/kraken/SRR1237783.kraken_species_rarefaction.tsv
        
[Wed Sep  7 15:38:56 2022]
Finished job 0.
1 of 1 steps (100%) done
