Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	kraken_species
	1

[Thu Sep  8 09:19:53 2022]
rule kraken_species:
    input: atavide.out/ReadAnnotations/SRR1237782/kraken/SRR1237782.report.tsv
    output: atavide.out/ReadAnnotations/SRR1237782/kraken/SRR1237782.species_fraction.tsv
    jobid: 0
    wildcards: sample=SRR1237782
    resources: mem_mb=2000, time=1440, load_superfocus=0, load_kraken=0, load_onehundred=0


        echo "Species	SRR1237782" > atavide.out/ReadAnnotations/SRR1237782/kraken/SRR1237782.species_fraction.tsv;
        cat atavide.out/ReadAnnotations/SRR1237782/kraken/SRR1237782.report.tsv | sed -e 's/Candidatus//' |         awk '{if ($4 == "S") {for (i=6; i<=NF; ++i) {printf "%s ", $i} printf "\t%f\n", $1}; }'         >> atavide.out/ReadAnnotations/SRR1237782/kraken/SRR1237782.species_fraction.tsv
        
[Thu Sep  8 09:19:54 2022]
Finished job 0.
1 of 1 steps (100%) done
